import torch
from smc.BootstrapFilter import RNNBootstrapFilter
from smc.utils import resample, resample_all_seq, log_gaussian_density_function, manual_log_density_function
import torch.nn as nn
import os
import numpy as np
import matplotlib.pyplot as plt
from train.utils import create_logger
import time
import math

# TODO: à checker
# std versus variance in different log_densities and torch.normal. OK DONE
# resample function in Backward IS. OK DONE.
# compute of log transition density by "batch"! OK DONE.
# formula for computing backward IS weights à checker. Right now: use q_k and not lk = g_k * q_k ...

class SmoothingAlgo:
    def __init__(self, bootstrap_filter, observations, out_folder,
                 logger=None):
        # '''
        # :param bootstrap_filter: Class Implementing a Bootstrap filter algorithm.
        # :param observations: sequence of observations generated by a stochastic RNN: tensor of shape (num_samples=B, num_particles, seq_len, output_size)
        # :param states: sequence of hidden states generated by a stochastic RNN: tensor of shape (num_samples, num_particles, seq_len, hidden_size)
        # :param backward_samples: number of backward_samples for the Backward IS Smoothing algo.
        # :param estimation_function: Fonction to estimate: in our case $mathbb[E][X_0|Y_{0:n}]$
        # '''
        self.bootstrap_filter = bootstrap_filter
        self.observations = observations  # Tensor of shape (B, particles, seq_len, output_size)
        self.num_particles = self.bootstrap_filter.num_particles
        if len(self.observations.shape) == 4:
            self.seq_len = self.observations.size(-2)
        else:
            self.seq_len = self.observations.size(-1)
        if logger is None:
            self.logger = self.create_logger(out_folder)
        else:
            self.logger = logger

    def create_logger(self, out_folder):
        if out_folder is not None:
            out_file_log = os.path.join(out_folder, 'debug_log.log')
            logger = create_logger(out_file_log=out_file_log)
            return logger
        else:
            return None

    def init_particles(self, params):
        self.ancestors = torch.normal(
                mean=torch.zeros(self.num_particles, 1),
                std=params[0]/math.sqrt(1-params[0]**2)) # change with N(0, alpha**2/1-\alpha**2) ?, # shape (P,1)
        self.trajectories = self.ancestors.unsqueeze(-2)
        self.filtering_weights = self.bootstrap_filter.compute_filtering_weights(particle=self.ancestors,
                                                                                 observation=self.observations[:,
                                                                                             0], params=params)  # decide if take $Y_0 of $Y_1$
        self.past_tau = torch.zeros(self.num_particles, 1)
        self.new_tau = self.past_tau
        self.taus = []
        self.all_IS_weights = []



class SVBackwardISSmoothing(SmoothingAlgo):
    def __init__(self, bootstrap_filter, observations, backward_samples, out_folder=None,
                 save_elements=False, logger=None, index_state=None):
        # '''
        # :param bootstrap_filter: Class Implementing a Bootstrap filter algorithm.
        # :param observations: sequence of observations generated by a stochastic RNN: tensor of shape (num_samples=B, num_particles, seq_len, output_size)
        # :param backward_samples: number of backward_samples for the Backward IS Smoothing algo.
        # '''
        super(SVBackwardISSmoothing, self).__init__(bootstrap_filter=bootstrap_filter, observations=observations,
                                                    out_folder=out_folder, logger=logger)
        self.backward_samples = backward_samples
        self.save_elements = save_elements
        self.index_state = index_state
        if self.index_state is not None:
            print("STATE ESTIMATION")
            self.estimation_function = self.state_estimation_function
        else:
            print("PARAMETER ESTIMATION")
            self.estimation_function = self.log_density_estimation_function

    def state_estimation_function(self, ancestors, k, particle=None, next_observation=None, params=None):
        if k == self.index_state:
            out = ancestors
        else:
            out = ancestors.new_zeros(ancestors.size())
        return out


    def log_density_estimation_function(self, ancestors, particle, next_observation, params, k=None):
        # next observation : shape (P)
        # particle: shape (P,1)
        particle = particle.unsqueeze(1).repeat((1, self.backward_samples, 1)) # shape (P,J,1)
        next_observation = next_observation.view(-1, 1, 1).repeat((1, self.backward_samples, 1)) # shape (P,J,1)
        log_transition_density = log_gaussian_density_function(X=particle,
                                                               mean=params[0] * ancestors,
                                                               covariance=params[1] ** 2)
        # log_t2 = manual_log_density_function(X=particle, mean=self.bootstrap_filter.params[0]*ancestors, covariance=self.bootstrap_filter.params[1]**2*torch.eye(1))
        covariance = torch.exp(particle).unsqueeze(-1)  # shape (B,J,1,1)
        log_observation_density = log_gaussian_density_function(X=next_observation,
                                                                mean=torch.zeros(size=next_observation.size()),
                                                                covariance=params[
                                                                               2] ** 2 * covariance)
        # log_o2 = manual_log_density_function(X=next_observation, mean=torch.zeros(size=next_observation.size()), covariance=self.bootstrap_filter.params[2]**2*covariance)
        return (log_transition_density + log_observation_density).unsqueeze(-1)

    def update_tau(self, ancestors, particle, backward_indices, IS_weights, next_observation, params, k=None):
        # '''
        # :param ancestors: ancestors particles $\xi_{k-1}^Jk$ sampled with backward_indices. tensor of shape (B, backward_samples, hidden_size)
        # :param particle: $\xi_k^l$ (here not used in the formula of the estimation function): tensor of shape (B, 1, hidden_size)
        # :param backward_indices: $J_k(j)$: tensor of shape (B, backward_samples)
        # :param IS_weights: normalized importance sampling weights: tensor of shape (B, backward_samples)
        # :param k: current timestep.
        # '''
        # '''update $\tau_k^l from $\tau_{k-1}^l, $w_{k-1]^l, $\xi_{k-1}^Jk$ and from Jk(j), \Tilde(w)(l,j) for all j in 0...backward samples'''

        resampled_tau = resample(self.past_tau.repeat(backward_indices.size(0), 1, 1),
                                 backward_indices)  # (particles,backward_samples, 1) # OK, function resampled checked.

        new_estimate = self.estimation_function(ancestors=ancestors, particle=particle,
                                                next_observation=next_observation, params=params, k=k)  # shape (P,J,1)

        new_tau_element = IS_weights * (
                    resampled_tau + new_estimate)  # shape (P,J,1)  # (particles, backward_samples, 1)
        new_tau = new_tau_element.sum(1)
        return new_tau  # shape (P,1)

    def estimate_conditional_expectation_of_function(self, params):
        self.init_particles(self.bootstrap_filter.params)
        with torch.no_grad():
            # for loop on time
            for k in range(self.seq_len):
                # Run bootstrap filter at time k
                self.old_filtering_weights = self.filtering_weights  # w_k. # shape (P)
                self.past_tau = self.new_tau # shape (P,1)
                self.particles, self.filtering_weights = self.bootstrap_filter.get_new_particle(
                    next_observation=self.observations[:, k],
                    ancestor=self.ancestors, weights=self.old_filtering_weights,
                    params=self.bootstrap_filter.params)  #particle = \xi_{k+1}, ancestors = \xi_k

                # Backward Simulation
                # A. Get backward Indice J_{k+1} from past filtering weights w_k
                backward_indices = torch.multinomial(self.old_filtering_weights,
                                                     self.num_particles * self.backward_samples, replacement=True)
                backward_indices = backward_indices.view(self.num_particles, self.backward_samples)  # shape (B, J)

                # B. Select Ancestor \xi_k^{J_{k+1}} with J.
                ancestors = resample(self.ancestors.repeat(backward_indices.size(0), 1, 1),
                                     backward_indices)  # shape (P, J, 1) # ok FUNCTION RESAMPLE CHECKED.
                # C. Compute IS weights \bar{\omega}_k with resampled Ancestor \xi_k^{J_{k+1}}  & Particle \xi_{k+1}# shape (P,J)
                is_weights = self.bootstrap_filter.compute_IS_weights(resampled_ancestors=ancestors,
                                                                      particle=self.particles,
                                                                      next_observation=self.observations[:, k],
                                                                      backward_samples=self.backward_samples,
                                                                      params=self.bootstrap_filter.params) # shape (P,J)

                # compute $\tau_{k+1}$ with all backward IS weights \bar{\omega}_k,  resampled ancestors \xi_k^{J_{k+1}}, current particle  \xi_{k+1} & all backward_indices J_{k+1}.
                new_tau = self.update_tau(ancestors=ancestors, particle=self.particles,
                                          backward_indices=backward_indices,
                                          IS_weights=is_weights.unsqueeze(-1),
                                          next_observation=self.observations[:, k], params=params, k=k)
                self.new_tau = new_tau
                self.ancestors = self.particles
                self.taus.append(self.new_tau)

        return -((self.new_tau.squeeze()*self.filtering_weights).sum().numpy())
